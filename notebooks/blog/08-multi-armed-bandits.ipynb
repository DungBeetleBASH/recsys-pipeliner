{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-armed bandits for real-time testing of recommenders\n",
    "\n",
    "I had previously read about contextual bandits in the [Sakemaker SDK docs](https://sagemaker-examples.readthedocs.io/en/latest/reinforcement_learning/bandits_statlog_vw_customEnv/bandits_statlog_vw_customEnv.html). But recently, I saw Alan Thawley's talk at the 2025 AWS Summit in London on how PokerStars test and deploy new recommender models in realtime using Kafka, Flink and multi-armed bandits. I've been curious about stream-based architectures for a while. Conceptually they're very similar to [ReactiveX](https://reactivex.io/) observable streams, which I've really enjoyed working with in the past. But the thing that really grabbed my attention was seeing how PokerStars use bandits to quickly and safely roll out the deployment of new recommender models.\n",
    "\n",
    "In Alan's talk, the bandit was just a little orange box in a much larger architecture diagram, though he did a great job of explaining their purpose in the system. \n",
    "\n",
    "Following on from this I spent a lot of time digging into the the AWS contextual bandits implementation, which is a wrapper around the [Vowpal Wabbit](https://vowpalwabbit.org/) ML library. The AWS container seems to not be actively maintained any more, and not that well documented either. As far as I can see, AWS haven't put much focus into this area in recent years.\n",
    "\n",
    "The main contendor for building multi-armed bandits seems to be [Tensorflow Agents](https://www.tensorflow.org/agents), though I have a few alternatives to investigate too:\n",
    "\n",
    "- https://github.com/criteo-research/bandit-reco\n",
    "- https://github.com/criteo-research/reco-gym\n",
    "- https://github.com/Farama-Foundation/Gymnasium\n",
    "- https://zr-obp.readthedocs.io/en/latest/\n",
    "- https://github.com/SforAiDl/genrl\n",
    "\n",
    "To focus on the bandit problem here, I'm going to start by building a simulated recommendations with user feedback. There is probably a much better way of doing this (even in the links above), but I'm a novice in this area and need something a bit more concrete to start.\n",
    "\n",
    "## The recommendations architecture\n",
    "\n",
    "![Recommendations environment](./assets/bandits-simulated-env.drawio.png)\n",
    "\n",
    "## Infinite Armed Bandit\n",
    "\n",
    "It will be necessary to use an \"infinite-armed bandit\", because the number of models to chose from will change over time as new recommender models are added or retired. Infinite armed bandits require attributes of the models as input to the bandit, and the bandit selects actions from a potentially infinite latent space. \n",
    "\n",
    "Attributes to use for the vectors representing each model could include:\n",
    "\n",
    "- model type\n",
    "- model version\n",
    "- model hyperparameters\n",
    "- model [validation metric](https://www.evidentlyai.com/ranking-metrics/evaluating-recommender-systems) results, such as:\n",
    "   - Hit rate\n",
    "   - Precision/Recall/F1 @ K\n",
    "   - Diversity\n",
    "   - Novelty\n",
    "\n",
    "Sources to follow up:\n",
    "\n",
    "- [Reinforcement Learning â€“ An Introduction](https://www.amazon.co.uk/dp/0262039249/?coliid=I269KGYUGCYM0G)\n",
    "- [Bandit Algorithms](https://www.amazon.co.uk/dp/1108486827/?coliid=IGOPZPO7LM7T7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
