{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p notebooks/pipelines/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from sagemaker import Session\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from pipeliner.exceptions import SagemakerSessionException\n",
    "from pipeliner.sagemaker.session import create_pipeline_session\n",
    "from pipeliner.sagemaker.pipeline import PipelineFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notebooks/pipelines/code/transform.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile notebooks/pipelines/code/transform.py\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "# # Since we get a headerless CSV file, we specify the column names here.\n",
    "# feature_columns_names = [\n",
    "#     \"sex\",\n",
    "#     \"length\",\n",
    "#     \"diameter\",\n",
    "#     \"height\",\n",
    "#     \"whole_weight\",\n",
    "#     \"shucked_weight\",\n",
    "#     \"viscera_weight\",\n",
    "#     \"shell_weight\",\n",
    "# ]\n",
    "# label_column = \"rings\"\n",
    "\n",
    "# feature_columns_dtype = {\n",
    "#     \"sex\": str,\n",
    "#     \"length\": np.float64,\n",
    "#     \"diameter\": np.float64,\n",
    "#     \"height\": np.float64,\n",
    "#     \"whole_weight\": np.float64,\n",
    "#     \"shucked_weight\": np.float64,\n",
    "#     \"viscera_weight\": np.float64,\n",
    "#     \"shell_weight\": np.float64,\n",
    "# }\n",
    "# label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "\n",
    "# def merge_two_dicts(x, y):\n",
    "#     z = x.copy()\n",
    "#     z.update(y)\n",
    "#     return z\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "#     df = pd.read_csv(\n",
    "#         f\"{base_dir}/input/abalone-dataset.csv\",\n",
    "#         header=None,\n",
    "#         names=feature_columns_names + [label_column],\n",
    "#         dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    "#     )\n",
    "#     numeric_features = list(feature_columns_names)\n",
    "#     numeric_features.remove(\"sex\")\n",
    "#     numeric_transformer = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "#             (\"scaler\", StandardScaler()),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     categorical_features = [\"sex\"]\n",
    "#     categorical_transformer = Pipeline(\n",
    "#         steps=[\n",
    "#             (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "#             (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     preprocess = ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num\", numeric_transformer, numeric_features),\n",
    "#             (\"cat\", categorical_transformer, categorical_features),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     y = df.pop(\"rings\")\n",
    "#     X_pre = preprocess.fit_transform(df)\n",
    "#     y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "#     X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "#     np.random.shuffle(X)\n",
    "#     train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "\n",
    "#     pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "#     pd.DataFrame(validation).to_csv(\n",
    "#         f\"{base_dir}/validation/validation.csv\", header=False, index=False\n",
    "#     )\n",
    "#     pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting notebooks/pipelines/code/recommender_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile notebooks/pipelines/code/recommender_pipeline.py\n",
    "import sagemaker\n",
    "from sagemaker import ScriptProcessor\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "from pipeliner.factory import SagemakerPipelineFactory\n",
    "\n",
    "\n",
    "class RecommenderPipeline(SagemakerPipelineFactory):\n",
    "    local: bool\n",
    "\n",
    "    def create(\n",
    "        self,\n",
    "        role: str,\n",
    "        name: str,\n",
    "        session: sagemaker.Session,\n",
    "    ) -> Pipeline:\n",
    "        self.local = isinstance(session, LocalPipelineSession)\n",
    "\n",
    "        instance_type = ParameterString(\n",
    "            name=\"InstanceType\",\n",
    "            default_value=\"local\" if self.local else \"ml.m5.large\",\n",
    "        )\n",
    "\n",
    "        image_uri = sagemaker.image_uris.retrieve(\n",
    "            framework=\"sklearn\",\n",
    "            region=session.boto_region_name,\n",
    "            version=\"1.2-1\",\n",
    "        )\n",
    "\n",
    "        # Create a ScriptProcessor and add code / run parameters\n",
    "        processor = ScriptProcessor(\n",
    "            image_uri=image_uri,\n",
    "            command=[\"python3\"],\n",
    "            instance_type=instance_type,\n",
    "            instance_count=1,\n",
    "            role=role,\n",
    "            sagemaker_session=session,\n",
    "        )\n",
    "\n",
    "        processing_step = ProcessingStep(\n",
    "            name=\"processing-example\",\n",
    "            step_args=processor.run(\n",
    "                code=\"pipelines/sources/example_pipeline/evaluate.py\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return Pipeline(\n",
    "            name=name,\n",
    "            steps=[processing_step],\n",
    "            sagemaker_session=session,\n",
    "            parameters=[instance_type],\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
