{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create/update a `.env` file in the project root including the following environment variables.\n",
    "\n",
    "AWS_DEFAULT_SAGEMAKER_BUCKET\n",
    "\n",
    "AWS_PROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ../ --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m build ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dist_path = \"../dist\"\n",
    "package_path = os.path.join(dist_path, [f for f in os.listdir(dist_path) if f.endswith(\".tar.gz\")][0])\n",
    "package_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "AWS_DEFAULT_SAGEMAKER_BUCKET = os.environ.get(\"AWS_DEFAULT_SAGEMAKER_BUCKET\", None)\n",
    "DEFAULT_BUCKET_PREFIX = \"pipelines\"\n",
    "if AWS_DEFAULT_SAGEMAKER_BUCKET is None:\n",
    "    raise ValueError(\"AWS_DEFAULT_SAGEMAKER_BUCKET is not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log into Docker registry with ECR credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region eu-west-1 | docker login --username AWS --password-stdin 141502667606.dkr.ecr.eu-west-1.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p pipelines/recommendations/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipelines/recommendations/code/user_item_matrix_transformer.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class UserItemMatrixTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    This class is a custom scikit-learn transformer\n",
    "    that accepts a pandas dataframe of user/item interactions\n",
    "    and returns a user/item matrix.\n",
    "\n",
    "    :param user (str): Column name for user id\n",
    "    :param item (str): Column name for item id\n",
    "    :param rating (float): Column name for user/item rating\n",
    "    :param agg (str): Panadas aggregation function to use when combining duplicate user/item interactions\n",
    "    :param binary (bool): If True, user/item interactions are converted to binary values in the user/item output matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, user=\"user_id\", item=\"item_id\", rating=\"rating\", agg=\"max\", binary=False\n",
    "    ):\n",
    "        self.user = user\n",
    "        self.item = item\n",
    "        self.rating = rating\n",
    "        self.agg = agg\n",
    "        self.binary = binary\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        matrix = X.groupby([self.user, self.item])[self.rating].agg(self.agg).unstack()\n",
    "        if self.binary:\n",
    "            return matrix.notnull().astype(int)\n",
    "        else:\n",
    "            return matrix.fillna(0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "    data_types = {\"user_id\": str, \"item_id\": str, \"rating\": np.float64}\n",
    "\n",
    "    user_item_ratings = pd.read_csv(f\"{base_dir}/input/data/user_item_ratings.csv\", dtype=data_types, engine='python')\n",
    "    \n",
    "    transformer = UserItemMatrixTransformer()\n",
    "    user_item_matrix = transformer.transform(user_item_ratings)\n",
    "\n",
    "    user_item_matrix.to_csv(f\"{base_dir}/output/data/user_item_matrix.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipelines/recommendations/code/similarity_matrix_transformer.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import argparse\n",
    "\n",
    "class SimilarityTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    This class is a custom scikit-learn transformer\n",
    "    that accepts a user/item matrix where user ids are\n",
    "    the index and item ids are the columns and returns\n",
    "    a similarity matrix. It can be used to calculate\n",
    "    user-user or item-item similarity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kind=\"user\", metric=\"cosine\", normalise=False):\n",
    "        if kind not in [\"user\", \"item\"]:\n",
    "            raise ValueError(\"kind must be 'user' or 'item'\")\n",
    "        if metric not in [\"cosine\", \"dot\", \"euclidean\"]:\n",
    "            raise ValueError(\"metric must be 'cosine', 'dot', or 'euclidean'\")\n",
    "        self.kind = kind\n",
    "        self.metric = metric\n",
    "        self.normalise = normalise\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        matrix = X\n",
    "        if self.kind == \"item\":\n",
    "            matrix = X.T\n",
    "\n",
    "        if self.metric == \"cosine\":\n",
    "            df = pd.DataFrame(\n",
    "                cosine_similarity(matrix), index=matrix.index, columns=matrix.index\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only cosine similarity is currently supported\")\n",
    "\n",
    "        if self.normalise:\n",
    "            df = (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--kind\", type=str, default=\"user\")\n",
    "    parser.add_argument(\"--metric\", type=str, default=\"cosine\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "    user_item_matrix = pd.read_csv(f\"{base_dir}/input/data/user_item_matrix.csv\", dtype=np.float64)\n",
    "    \n",
    "    transformer = SimilarityTransformer(kind=args.kind, metric=args.metric)\n",
    "    similarity_matrix = transformer.transform(user_item_matrix)\n",
    "\n",
    "    similarity_matrix.to_csv(f\"{base_dir}/output/data/{args.kind}_similarity_matrix.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipelines/recommendations/code/item_based_recommender.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "import joblib\n",
    "from collections.abc import Sequence\n",
    "\n",
    "\n",
    "class ItemBasedRecommender(BaseEstimator):\n",
    "    \"\"\"Item-based collaborative filtering recommender.\"\"\"\n",
    "\n",
    "    n: int\n",
    "    threshold: float\n",
    "    similarity_matrix: pd.DataFrame\n",
    "    user_item_matrix: pd.DataFrame\n",
    "\n",
    "    def __init__(self, n=5, threshold=0.1):\n",
    "        self.n = n\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X: pd.DataFrame | tuple[pd.DataFrame, pd.DataFrame], y=None):\n",
    "        \"\"\"Fits the recommender to the given data.\n",
    "\n",
    "        Args:\n",
    "          X (pd.DataFrame | tuple[pd.DataFrame, pd.DataFrame]):\n",
    "            Single DataFrame with similarity matrix\n",
    "            or tuple of (similarity matrix, user/item matrix)\n",
    "        \"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.similarity_matrix = X\n",
    "        elif isinstance(X, tuple):\n",
    "            self.similarity_matrix = X[0]\n",
    "            self.user_item_matrix = X[1]\n",
    "        else:\n",
    "            raise ValueError(\"Input should be DataFrame or (DataFrame, DataFrame)\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _get_exclusions(self, item_id: str, user_id: str | None) -> list[str]:\n",
    "        if user_id is None:\n",
    "            return [item_id]\n",
    "        single_user_matrix = self.user_item_matrix.loc[user_id]\n",
    "        user_rated_items = single_user_matrix[single_user_matrix > 0]\n",
    "        return [item_id] + user_rated_items.index.to_list()\n",
    "\n",
    "    def _get_recommendations(self, item: str | tuple[str, str]) -> np.array:\n",
    "        if isinstance(item, str):\n",
    "            item_id, user_id = item, None\n",
    "        elif isinstance(item, tuple):\n",
    "            item_id, user_id = item[0], item[1]\n",
    "        else:\n",
    "            raise ValueError(\"Input items should be str or (str, str)\")\n",
    "\n",
    "        exclusions = self._get_exclusions(item_id, user_id)\n",
    "\n",
    "        item_recommendations = (\n",
    "            self.similarity_matrix[self.similarity_matrix[item_id] > self.threshold][\n",
    "                item_id\n",
    "            ]\n",
    "            .drop(exclusions, errors=\"ignore\")\n",
    "            .sort_values(ascending=False)\n",
    "        )\n",
    "        return np.array(item_recommendations.head(self.n).index)\n",
    "\n",
    "    def predict(self, X: Sequence[str] | Sequence[tuple[str, str]]) -> np.array:\n",
    "        \"\"\"Predicts n item recommendations for each item_id provided\n",
    "        If tuples of (user_id, item_id) are provided, items previously\n",
    "        rated by the user will be excluded from the recommendations.\n",
    "\n",
    "        Args:\n",
    "          X (Sequence): List of item_id or (item_id, user_id)\n",
    "\n",
    "        Returns:\n",
    "          np.array of shape (X.shape[0], n)\n",
    "        \"\"\"\n",
    "        return np.array([self._get_recommendations(item) for item in X])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "    user_item_matrix = pd.read_csv(f\"{base_dir}/input/data/user_item_matrix.csv\", dtype=np.float64)\n",
    "    similarity_matrix = pd.read_csv(f\"{base_dir}/input/data/user_similarity_matrix.csv\", dtype=np.float64)\n",
    "    \n",
    "    rec = UserBasedRecommender(5, 5, 0.1)\n",
    "    rec.fit((similarity_matrix, user_item_matrix))\n",
    "\n",
    "    joblib.dump(rec, os.path.join(f\"{base_dir}/output/model/\", \"rec.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pipelines/recommendations/code/user_based_recommender.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "import joblib\n",
    "from collections.abc import Sequence\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class UserBasedRecommender(BaseEstimator):\n",
    "    \"\"\"User-based collaborative filtering recommender.\"\"\"\n",
    "\n",
    "    n: int\n",
    "    n_users: int\n",
    "    threshold: float\n",
    "    similarity_matrix: pd.DataFrame\n",
    "    user_item_matrix: pd.DataFrame\n",
    "\n",
    "    def __init__(self, n=5, n_users=5, threshold=0.1):\n",
    "        self.n = n\n",
    "        self.n_users = n_users\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits the recommender to the given data.\n",
    "\n",
    "        Args:\n",
    "          X (tuple[pd.DataFrame, pd.DataFrame]):\n",
    "            tuple of (similarity matrix, user/item matrix)\n",
    "        \"\"\"\n",
    "        if isinstance(X, tuple):\n",
    "            self.similarity_matrix = X[0]\n",
    "            self.user_item_matrix = X[1]\n",
    "        else:\n",
    "            raise ValueError(\"Input should be tuple of (DataFrame, DataFrame)\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _get_similar_users(self, user_id: str):\n",
    "        return (\n",
    "            self.similarity_matrix[self.similarity_matrix[user_id] > self.threshold][\n",
    "                user_id\n",
    "            ]\n",
    "            .drop(user_id, errors=\"ignore\")\n",
    "            .sort_values(ascending=False)\n",
    "        )\n",
    "\n",
    "    def _get_exclusions(self, user_id):\n",
    "        single_user_matrix = self.user_item_matrix.loc[user_id]\n",
    "        user_rated_items = single_user_matrix[single_user_matrix > 0]\n",
    "        return user_rated_items.index.to_list()\n",
    "\n",
    "    def _get_recommendations(self, user_id):\n",
    "        if not isinstance(user_id, str):\n",
    "            raise ValueError(\"Input items should be str\")\n",
    "        exclusions = self._get_exclusions(user_id)\n",
    "        similar_users = self._get_similar_users(user_id)\n",
    "        matrix = self.user_item_matrix.T[similar_users.head(self.n_users).index]\n",
    "\n",
    "        user_recommendations = (\n",
    "            matrix[~matrix.index.isin(exclusions) & (matrix > 0).any(axis=\"columns\")]\n",
    "            .max(axis=1)\n",
    "            .sort_values(ascending=False)\n",
    "        )\n",
    "\n",
    "        return np.array(user_recommendations.head(self.n).index)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predicts n item recommendations for each user_id provided.\n",
    "\n",
    "        Args:\n",
    "          X (Sequence): List of user_id\n",
    "\n",
    "        Returns:\n",
    "          np.array of shape (X.shape[0], n)\n",
    "        \"\"\"\n",
    "        return np.array([self._get_recommendations(item) for item in X])\n",
    "\n",
    "    # def predict_proba(self, X):\n",
    "    #     raise NotImplementedError(\"predict_proba not implemented yet\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\"))\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--input\", type=str, default=os.environ.get(\"SM_INPUT_DIR\"))\n",
    "    parser.add_argument(\"--output\", type=str, default=os.environ.get(\"SM_OUTPUT_DIR\"))\n",
    "\n",
    "    \n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    logging.info(f\"SM_OUTPUT_DATA_DIR: {args.output_data_dir}\")\n",
    "    logging.info(f\"SM_MODEL_DIR: {args.model_dir}\")\n",
    "    logging.info(f\"SM_CHANNEL_TRAIN: {args.train}\")\n",
    "    logging.info(f\"SM_INPUT_DIR: {args.input}\")\n",
    "    logging.info(f\"SM_OUTPUT_DIR: {args.output}\")\n",
    "    \n",
    "    base_dir = \"/opt/ml\"\n",
    "    \n",
    "    logging.info(os.listdir(base_dir))\n",
    "    logging.info(os.listdir(args.input))\n",
    "    logging.info(os.listdir(f\"{args.input}/data\"))\n",
    "\n",
    "    user_item_matrix = pd.read_csv(f\"{args.input}/data/user_item_matrix/user_item_matrix.csv\", dtype=np.float64)\n",
    "    similarity_matrix = pd.read_csv(f\"{args.input}/data/similarity_matrix/user_similarity_matrix.csv\", dtype=np.float64)\n",
    "    \n",
    "    rec = UserBasedRecommender(5, 5, 0.1).fit((similarity_matrix, user_item_matrix))\n",
    "\n",
    "    joblib.dump(rec, os.path.join(args.model_dir, \"rec.joblib\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    role = iam.get_role(RoleName=\"pipeliner\")[\"Role\"][\"Arn\"]\n",
    "\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "\n",
    "session = LocalPipelineSession(\n",
    "    default_bucket=AWS_DEFAULT_SAGEMAKER_BUCKET,\n",
    "    default_bucket_prefix=DEFAULT_BUCKET_PREFIX,\n",
    ")\n",
    "session.config = {\"local\": {\"local_code\": True}}\n",
    "\n",
    "region = session.boto_region_name\n",
    "default_bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ratings_data_path = \"../tests/test_data/user_item_ratings.csv\"\n",
    "data_types = {\"user_id\": str, \"item_id\": str, \"rating\": np.float64}\n",
    "\n",
    "user_item_ratings = pd.read_csv(ratings_data_path, dtype=data_types, engine='python')\n",
    "user_item_ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=ratings_data_path,\n",
    "    desired_s3_uri=f\"s3://{default_bucket}/{DEFAULT_BUCKET_PREFIX}/recommender/data\",\n",
    ")\n",
    "input_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession\n",
    "from sagemaker.workflow.steps import CacheConfig, ProcessingStep, TrainingStep\n",
    "\n",
    "\n",
    "class RecommenderPipeline:\n",
    "    def create(\n",
    "        self,\n",
    "        role: str,\n",
    "        name: str,\n",
    "        session: sagemaker.Session,\n",
    "        framework_version = \"1.2-1\",\n",
    "    ) -> Pipeline:\n",
    "        self.local = isinstance(session, LocalPipelineSession)\n",
    "        self.framework_version = framework_version\n",
    "\n",
    "        instance_type = ParameterString(\n",
    "            name=\"InstanceType\",\n",
    "            default_value=\"local\" if self.local else \"ml.m5.large\",\n",
    "        )\n",
    "\n",
    "        input_data = ParameterString(\n",
    "            name=\"user_item_ratings\",\n",
    "            default_value=input_data_uri,\n",
    "        )\n",
    "\n",
    "        image_uri = sagemaker.image_uris.retrieve(\n",
    "            framework=\"sklearn\",\n",
    "            region=session.boto_region_name,\n",
    "            version=\"1.2-1\",\n",
    "        )\n",
    "\n",
    "        cache_config = CacheConfig(\n",
    "            enable_caching=True,\n",
    "            expire_after=\"P30d\",  # 30 days\n",
    "        )\n",
    "\n",
    "        processor = SKLearnProcessor(\n",
    "            framework_version=framework_version,\n",
    "            instance_type=instance_type,\n",
    "            instance_count=1,\n",
    "            base_job_name=\"sklearn-preprocess\",\n",
    "            role=role,\n",
    "            sagemaker_session=session,\n",
    "        )\n",
    "\n",
    "        user_item_matrix_step = ProcessingStep(\n",
    "            name=\"user_item_matrix_transformer\",\n",
    "            step_args=processor.run(\n",
    "                inputs=[\n",
    "                    ProcessingInput(\n",
    "                        source=input_data,\n",
    "                        input_name=\"user_item_ratings\",\n",
    "                        destination=\"/opt/ml/processing/input/data\",\n",
    "                    ),\n",
    "                    ProcessingInput(\n",
    "                        source=\"../src/pipeliner\",\n",
    "                        input_name=\"pipeliner\",\n",
    "                        destination=\"/opt/ml/processing/input/code/pipeliner\",\n",
    "                    ),\n",
    "                ],\n",
    "                outputs=[\n",
    "                    ProcessingOutput(\n",
    "                        output_name=\"user_item_matrix\",\n",
    "                        source=\"/opt/ml/processing/output/data\",\n",
    "                    ),\n",
    "                ],\n",
    "                code=\"pipelines/recommendations/code/user_item_matrix_transformer.py\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        user_similarity_matrix_step = ProcessingStep(\n",
    "            name=\"user_similarity_matrix_transformer\",\n",
    "            step_args=processor.run(\n",
    "                inputs=[\n",
    "                    ProcessingInput(\n",
    "                        source=user_item_matrix_step.properties.ProcessingOutputConfig.Outputs[\"user_item_matrix\"].S3Output.S3Uri,\n",
    "                        input_name=\"user_item_matrix\",\n",
    "                        destination=\"/opt/ml/processing/input/data\"),\n",
    "                ],\n",
    "                outputs=[\n",
    "                    ProcessingOutput(\n",
    "                        output_name=\"user_similarity_matrix\", \n",
    "                        source=\"/opt/ml/processing/output/data\"),\n",
    "                ],\n",
    "                code=\"pipelines/recommendations/code/similarity_matrix_transformer.py\",\n",
    "            ),\n",
    "            job_arguments=[\"--kind\", \"user\"],\n",
    "        )\n",
    "\n",
    "        item_similarity_matrix_step = ProcessingStep(\n",
    "            name=\"item_similarity_matrix_transformer\",\n",
    "            step_args=processor.run(\n",
    "                inputs=[\n",
    "                    ProcessingInput(\n",
    "                        source=user_item_matrix_step.properties.ProcessingOutputConfig.Outputs[\"user_item_matrix\"].S3Output.S3Uri,\n",
    "                        input_name=\"user_item_matrix\",\n",
    "                        destination=\"/opt/ml/processing/input/data\"),\n",
    "                ],\n",
    "                outputs=[\n",
    "                    ProcessingOutput(\n",
    "                        output_name=\"item_similarity_matrix\", \n",
    "                        source=\"/opt/ml/processing/output/data\"),\n",
    "                ],\n",
    "                code=\"pipelines/recommendations/code/similarity_matrix_transformer.py\",\n",
    "            ),\n",
    "            job_arguments=[\"--kind\", \"item\"],\n",
    "        )\n",
    "\n",
    "        sklearn_estimator = SKLearn(\n",
    "            entry_point=\"pipelines/recommendations/code/user_based_recommender.py\",\n",
    "            role=role,\n",
    "            image_uri=image_uri,\n",
    "            instance_type=instance_type,\n",
    "            sagemaker_session=session,\n",
    "            base_job_name=\"training_job\",\n",
    "            # hyperparameters=hyperparameters,\n",
    "            enable_sagemaker_metrics=True,\n",
    "        )\n",
    "\n",
    "        training_step = TrainingStep(\n",
    "            name=\"Train\", \n",
    "            estimator=sklearn_estimator, \n",
    "            cache_config=cache_config,\n",
    "            inputs={\n",
    "                \"user_item_matrix\": TrainingInput(\n",
    "                    s3_data=user_item_matrix_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"user_item_matrix\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"similarity_matrix\": TrainingInput(\n",
    "                    s3_data=user_similarity_matrix_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"user_similarity_matrix\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return Pipeline(\n",
    "            name=name,\n",
    "            steps=[\n",
    "                user_item_matrix_step, \n",
    "                user_similarity_matrix_step, \n",
    "                item_similarity_matrix_step,\n",
    "                training_step\n",
    "            ],\n",
    "            sagemaker_session=session,\n",
    "            parameters=[input_data, instance_type],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = RecommenderPipeline().create(role=role, name=\"recommender\", session=session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = execution.list_steps()\n",
    "steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
